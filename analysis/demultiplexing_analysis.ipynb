{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index pairs from stats file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('../output/Stats/DemultiplexingStats.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "index_pairs = []\n",
    "\n",
    "for index_pair in root.iter('Barcode'):\n",
    "  name = index_pair.get('name')\n",
    "  if name != \"all\":\n",
    "    index_pairs.append((name, int(index_pair[0].find('BarcodeCount').text)))\n",
    "\n",
    "index_pairs = sorted(index_pairs, key=lambda x: x[1], reverse=True)\n",
    "index_pairs_present = [index_pair for index_pair in index_pairs if index_pair[1]]\n",
    "\n",
    "# index_pairs\n",
    "# index_pairs_present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results from GSC qc run and compare to our demultiplexing run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSC_results = pd.read_csv('../data/GSC_demultiplex_stats.csv')\n",
    "# GSC_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that our counts match the GSC counts for each index pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = [\"{:<20} {:<10} {:<10} {:<10}\".format(*['Barcode', 'Our Count', 'GSC Count', 'Equal'])]\n",
    "for index_pair, count in index_pairs_present:\n",
    "  if index_pair in list(GSC_results['Barcode sequence']):\n",
    "    GSC_count = int((GSC_results.loc[GSC_results['Barcode sequence'] == index_pair].iloc[0]['PF Clusters']).replace(',', ''))\n",
    "    comparison.append(\"{:<20} {:<10} {:<10} {:<10}\".format(*[index_pair, count, GSC_count, count == GSC_count]))\n",
    "\n",
    "print('\\n'.join(comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 'unknown' has different counts. This is because we gave them incorrect indexes for 9 of the libraries, so their reads were part of the 'unknown' counts when GSC first tried demultiplexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all barcodes that were not demultiplexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../output/Undetermined_S0_L001_R1_001.fastq.gz', 'rb') as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "discarded = []\n",
    "for line in lines:\n",
    "  line = line.decode('utf-8')\n",
    "  if line[:7] == '@M00329':\n",
    "    discarded.append(line[-18:-1])\n",
    "\n",
    "print(len(discarded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39915 reads didn't demultiplex. This matches the \"unknown\" count. A good portion of these discarded reads are PhiX pike-in, which had the index pair ATCTCGTA+TCTTTCCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_counts = Counter(discarded)\n",
    "print(discarded_counts['ATCTCGTA+TCTTTCCC'])\n",
    "discarded_unique = set(discarded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11313 reads were from the PhiX with no mismatches in the barcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove PhiX index pair and any other index pairs from discarded_counts such that both indexes are within hamming distance of 1 from the true PhiX indexes (this simulates what bcl2fastq would do). Add the PhiX index and read counts to index_pairs_present. Also remove \"unknown\" from index_pairs_present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(a, b):\n",
    "  return(sum(i != j for i, j in zip(a, b)))\n",
    "\n",
    "correct_index1 = 'ATCTCGTA'\n",
    "correct_index2 = 'TCTTTCCC'\n",
    "s = 0\n",
    "phiX_discarded = []\n",
    "for discarded_pair in discarded_unique:\n",
    "  index1 = discarded_pair[:8]\n",
    "  index2 = discarded_pair[9:]\n",
    "  if hamming(correct_index1, index1) <= 1 and hamming(correct_index2, index2) <= 1:\n",
    "    s += discarded_counts[discarded_pair]\n",
    "    phiX_discarded.append(discarded_pair)\n",
    "\n",
    "print(len(index_pairs_present))\n",
    "index_pairs_present = [i for i in index_pairs_present if i[0] != 'unknown']\n",
    "print(len(index_pairs_present))\n",
    "index_pairs_present.append(('ATCTCGTA+TCTTTCCC', s))\n",
    "\n",
    "print(s)\n",
    "print(len(phiX_discarded))\n",
    "\n",
    "print(len(discarded_unique))\n",
    "discarded_unique = [x for x in discarded_unique if x not in phiX_discarded]\n",
    "print(len(discarded_unique))\n",
    "\n",
    "print(len(discarded_counts))\n",
    "for pair in phiX_discarded:\n",
    "  discarded_counts.pop(pair)\n",
    "print(len(discarded_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many unique index pairs are present in the discarded reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(discarded_counts))\n",
    "# print(len(discarded_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many reads didn't demultiplex after removing PhiX reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(discarded_counts.total())\n",
    "print(discarded_counts.total() + s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28265 reads didn't demultiplex. Look at Distribution of read counts for index pairs that didn't demultiplex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(discarded_counts.values(), bins=range(12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(discarded_counts.values(), bins=range(3, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check how many discarded reads had index pairs that were the result of a deletion in one or both indexes. For MiSeq, a deletion in either index will result in an 'A' at the 8th position of the index. For each correct index pair, generate all index pair combinations that could result from a single deletion in one or both indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_deletions(seq):\n",
    "  deletions = [seq]\n",
    "  for i in range(len(seq)):\n",
    "    seq_bases = [base for base in seq]\n",
    "    seq_bases.pop(i)\n",
    "    seq_bases.append('A')\n",
    "    deletions.append(''.join(seq_bases))\n",
    "  return deletions\n",
    "\n",
    "# print(all_deletions('ACGT'))\n",
    "\n",
    "def all_deletion_combinations(idx1, idx2):\n",
    "  return [i + '+' + j for i, j in product(all_deletions(idx1), all_deletions(idx2))]\n",
    "\n",
    "# print(all_deletion_combinations('ACTG', 'GTCA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get deletion index pairs present in the discarded reads and the counts for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barcodes_present.append(('ATCTCGTA+TCTTTCCC', 0)) # phiX\n",
    "\n",
    "observed_deletions = {}\n",
    "\n",
    "for index_pair, frequency in index_pairs_present:\n",
    "  pair_observed_deletions = []\n",
    "  deletions = all_deletion_combinations(index_pair[:8], index_pair[9:])\n",
    "  for deletion in deletions:\n",
    "    if deletion in discarded_unique:\n",
    "      pair_observed_deletions.append((deletion, discarded_counts[deletion]))\n",
    "  # Using Counter here accounts for cases where the same index pair is present\n",
    "  # multiple times in the list of all possible combinations.\n",
    "  observed_deletions[index_pair] = Counter(dict(pair_observed_deletions))\n",
    "\n",
    "# observed_deletions['CTTCGGTC+CATCGTTA']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many discarded reads were the result of a deletion in one or both indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(pairs.total() for pairs in observed_deletions.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each index pair, find all discarded index pairs such that each index in the discarded pair is within Levenshtein distance of two from the correct index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_index_pairs = {}\n",
    "\n",
    "for index_pair, frequency in index_pairs_present:\n",
    "  similar_to_pair = []\n",
    "  correct_index1 = index_pair[:8]\n",
    "  correct_index2 = index_pair[9:]\n",
    "  for discarded_pair in discarded_unique:\n",
    "    index1 = discarded_pair[:8]\n",
    "    index2 = discarded_pair[9:]\n",
    "    if levenshtein(correct_index1, index1) <= 2 and levenshtein(correct_index2, index2) <= 2:\n",
    "      similar_to_pair.append((discarded_pair, discarded_counts[discarded_pair]))\n",
    "  similar_index_pairs[index_pair] = Counter(dict(similar_to_pair))\n",
    "\n",
    "# similar_index_pairs['CTTCGGTC+CATCGTTA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(pairs.total() for pairs in similar_index_pairs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_stats = pd.DataFrame(index_pairs_present, columns=['Index', 'Demultiplexed Reads'])\n",
    "index_stats = index_stats.drop(6) # get rid of unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_stats['Similar Index Count'] = [len(similar_index_pairs[index]) for index in index_stats['Index']]\n",
    "index_stats['Similar Index Reads'] = [similar_index_pairs[index].total() for index in index_stats['Index']]\n",
    "index_stats['Similar Index Percent'] = index_stats['Similar Index Reads']/index_stats['Demultiplexed Reads'] * 100\n",
    "\n",
    "index_stats['Deletion Index Count'] = [len(observed_deletions[index]) for index in index_stats['Index']]\n",
    "index_stats['Deletion Index Reads'] = [observed_deletions[index].total() for index in index_stats['Index']]\n",
    "index_stats['Deletion Index Percent'] = index_stats['Deletion Index Reads']/index_stats['Demultiplexed Reads'] * 100\n",
    "index_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PhiX barcode has a much lower frequency of deletions compared to the other barcodes: ~0.02% vs ~0.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(index_stats['Similar Index Reads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12713 of the 28265 discarded reads had an index pair within our Levenshtein distance threshold of the correct index sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa7c27dc793df5be2edaadabdf154ce9b78714d52aa15f6b4e8ee8c3cd646d7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
